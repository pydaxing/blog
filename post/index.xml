<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 冷眸</title>
    <link>https://blog.pydaxing.com/post/</link>
    <description>Recent content in Posts on 冷眸</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 24 Nov 2020 18:12:17 +0000</lastBuildDate><atom:link href="https://blog.pydaxing.com/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DKT复现详解</title>
      <link>https://blog.pydaxing.com/post/20201124_dkt-pytorch/</link>
      <pubDate>Tue, 24 Nov 2020 18:12:17 +0000</pubDate>
      
      <guid>https://blog.pydaxing.com/post/20201124_dkt-pytorch/</guid>
      <description>知识追踪（Knowledge Tracing）是根据学生过去的答题情况对学生的知识掌握情况进行建模，从而得到学生当前知识状态表示的一种技术。将深度学习的方法引入知识追踪最早出现于发表在NeurIPS 2015上的一篇论文《Deep Knowledge Tracing》，作者来自斯坦福大学。在这篇论文中，作者提出了使用深度知识追踪（Deep Knowledge Tracing, DKT）的概念，利用RNN对学生的学习情况进行建模，之后引出了一系列工作，2019年已经有使用Transformer代替RNN和LSTM并且达到了SOTA的论文。DKT作为知识追踪模型深度化的开山之作，在几乎所有的深度知识追踪模型中都作为baseline，而DKT作者给出的模型实现是基于lua语言的，为了能够让更多的研究人员更方便的使用，这里给出一种python的实现，采用的是pytorch框架。
下载 模型代码已经发布在github上，可点击这里查看和下载具体代码。
或者可以直接通过如下命令直接下载到本地：
git clone https://github.com/pydaxing/DeepKnowledgeTracing-DKT-Pytorch.git
具体运行和使用方法参考GitHub项目上ReadMe。
项目结构-DKT 在DKT文件夹下包括两个文件夹：KTDataset和KnowledgeTracing。
KTDataset文件夹下有6个常用的知识追踪数据集，数据都已经处理成三行格式：
第一行：答题数 第二行：题目编号 第三行：答题结果，0表示错，1表示对
Note：可根据需要，按照数据格式自行添加新的数据集。
模型结构-KnowledgeTracing 模型的整个流程都在KnowledgeTracing目录下，包括模型、参数设置、数据处理、模型训练和评估，分别在四个子目录下：model， Constant，data，evaluation。
参数设置-Constant Constant下主要设置一些参数和超参数，超参数也分为四大块：数据集存储路径、数据集、题目数、模型超参数。
数据集存储路径
Dpath = &amp;#39;../../KTDataset&amp;#39; 数据集：一共包括6个数据集
datasets = { &amp;#39;assist2009&amp;#39; : &amp;#39;assist2009&amp;#39;, &amp;#39;assist2015&amp;#39; : &amp;#39;assist2015&amp;#39;, &amp;#39;assist2017&amp;#39; : &amp;#39;assist2017&amp;#39;, &amp;#39;static2011&amp;#39; : &amp;#39;static2011&amp;#39;, &amp;#39;kddcup2010&amp;#39; : &amp;#39;kddcup2010&amp;#39;, &amp;#39;synthetic&amp;#39; : &amp;#39;synthetic&amp;#39; } 题目数：表示每个数据集里面题目的数量
numbers = { &amp;#39;assist2009&amp;#39; : 124, &amp;#39;assist2015&amp;#39; : 100, &amp;#39;assist2017&amp;#39; : 102, &amp;#39;static2011&amp;#39; : 1224, &amp;#39;kddcup2010&amp;#39; : 661, &amp;#39;synthetic&amp;#39; : 50 } 模型超参数：主要包括所用数据集、输入输出维度、学习率、最大步长、学习周期等。</description>
    </item>
    
    <item>
      <title>支持向量机</title>
      <link>https://blog.pydaxing.com/post/20171014_svm/</link>
      <pubDate>Sat, 14 Oct 2017 12:18:00 +0000</pubDate>
      
      <guid>https://blog.pydaxing.com/post/20171014_svm/</guid>
      <description>支持向量机（SVM）是一个非常好的监督学习算法，由于这个算法涉及到的数学知识比较多，而且不容易弄懂，因此我额外补了一些拉格朗日对偶性的一些知识，还初步了解了以下凸优化的相关知识，建议如过想学SVM的先去学一些拉格朗日对偶问题以及一些凸优化的相关东西。终于能大致弄懂SVM的整个过程中所涉及到的一些东西，但是仍有一些地方还是不太明白，还有待继续学习。这里先对所掌握的过程进行回顾总结。
Support Vector Machine (SVM) 支持向量机（SVM）是一个监督学习算法，它实质上是一种特征空间上的间隔最大的线性分类器，这其中涉及到核技巧将线性不可分问题映射成特征空间中的线性可分问题，因此支持向量机可以说是一个非线性分类器，它的学习策略就是间隔最大化，最后的问题可以转化为一个凸优化问题，使用拉格朗日对偶性来求解对偶问题，从而间接得到原问题的解，一般使用SMO算法来求对偶问题的解。
间隔最大化 支持向量机的思想就是间隔最大化，根据间隔最大化来构造原始问题。这里涉及到的间隔包括函数间隔和几何间隔，其中函数间隔为$\hat \gamma_i$ $$ \hat \gamma_i = y_i(\omega\cdot x_i + b) \tag{1} $$ 然后得到所有样本的函数间隔中的最小的一个$$\hat\gamma = \min_{i = 1,\cdots,N}(\hat\gamma_i)$$
几何间隔由空间中两点的实际距离定义记作$\gamma_i$ $$ \gamma_i = \dfrac{\omega}{||\omega||}\cdot x_i + \dfrac{b}{||\omega||}\tag{2} $$ 同理求出所有几何间隔最小的$$\gamma = \min_{i =1\cdots,N}(\gamma_i)$$
可以看出函数间隔是不唯一的，因为满足分离超平面$\omega^\ast\cdot x + b^\ast= 0$的参数$(\omega,b)$并不是唯一的，例如将$\omega$和$b$发你别扩大两倍后$2\omega^\ast\cdot x + 2b^\ast = 0$仍然成立，而函数间隔$\hat\gamma$会随着$(\omega , b)$变化。而几何距离却不会发生变化。
同时还可以看出函数间隔和几何间隔之间满足关系(3) $$ \gamma = \dfrac{\hat\gamma}{||\omega||}\tag{3} $$ 因此这里考虑求解几何间隔最大的分离超平面，该问题则表示为下面的约束最优化问题 $$ \max_{\omega,b} \quad \gamma\tag{4}\\ s.t.\quad y_i(\dfrac{\omega}{||\omega||}\cdot x_i + \dfrac{b}{||\omega||}) \ge \gamma,\qquad i = 1,2,\cdots,N $$ 根据上述关系(3)将问题转换为 $$ \max_{\omega,b} \quad \dfrac{\hat\gamma}{||\omega||}\tag{5}\\ s.</description>
    </item>
    
  </channel>
</rss>
