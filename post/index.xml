<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 冷眸</title>
    <link>https://blog.pydaxing.com/post/</link>
    <description>Recent content in Posts on 冷眸</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 18 Nov 2023 12:18:00 +0000</lastBuildDate><atom:link href="https://blog.pydaxing.com/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>深入解析LLM Text-2-SQL：从Prompt到Finetune全网最详尽攻略</title>
      <link>https://blog.pydaxing.com/post/20231119_nl2sql_prompt2finetune/</link>
      <pubDate>Sat, 18 Nov 2023 12:18:00 +0000</pubDate>
      
      <guid>https://blog.pydaxing.com/post/20231119_nl2sql_prompt2finetune/</guid>
      <description>1、LLM技术在Text-to-SQL上的现状 Text-to-SQL解决方案在大型语言模型（LLM）的支持下得到了广泛应用，但是现有的提示工程方法缺乏系统性，阻碍了该领域的发展。
为了更深入地探讨基于LLM的Text-to-SQL解决方案的提示工程系统化研究，我们需要全面理解和实践，覆盖了不同的LLM、问题表示、示例选择和组织策略，以探索如何与LLM良好配合。具体而言，提示工程分为zero-shot和few-shot场景，在zero-shot场景中，需要研究如何有效地表示自然语言问题，包括融入相关信息如相应数据库模式。在few-shot场景中，有限的示例可供使用，因此需要研究如何选择最有帮助的示例，并恰当地组织到提示中。
本文还探讨了LLM的上下文学习过程，并研究了zero-shot和few-shot场景中LLM的表现，同时也展示了有监督微调方法的利与弊。本文为基于大型语言模型的Text-to-SQL解决方案的系统化研究提供了有益的参考和指导。
2、问题表示 这里的问题表示是指针对Text-to-SQL任务，设计一个高效的Prompt去激发LLM的Text-to-SQL能力，本质上就是Prompt工程。在zero-shot场景中有五花八门的Prompt模版，这里展示现有文献中四个最具代表性的。此外，再展示一个在 Alpaca中使用的Prompt模板，因为它在监督式微调中非常受欢迎。下表总结了这五种方法：
问题表示 INS RI FK LLMs EM $\text{BS}_P$ x x x - - $\text{TR}_P$ $\checkmark$ x x Code-Davinci-002 69.0 $\text{OD}_P$ $\checkmark$ $\checkmark$ x GPT-3.5-Turbo GPT-4 70.1 $\text{CR}_P$ $\checkmark$ x $\checkmark$ Code-Davinci-002 GPT-3.5-Turbo 75.6 $\text{AS}_P$ $\checkmark$ x x - EX：SQL执行准确率、INS：指令（任务描述）、RI：规则信息（指导性语句，比如“仅输出SQL语句，无需解释”）、FK：外键（数据库的外键信息）
2.1、基本提示 基本提示（Basic Prompt）是一种简单的Prompt模板，它由表模式、以 Q: 为前缀的自然语言问题、以及提示LLM生成SQL的响应前缀 A:SELECT 组成。之所以命名为基本提示，是因为它并未包含任何指令内容。
Table table_name1, columns = [column1, column2, ...] ... Table table_name2, columns = [column1, column2, ...] Q: The question A: SELECT.</description>
    </item>
    
    <item>
      <title>Code LlaMA 是怎样练成的: 一文详解代码生成SOTA大模型</title>
      <link>https://blog.pydaxing.com/post/20231018_codellama/</link>
      <pubDate>Mon, 16 Oct 2023 19:12:17 +0000</pubDate>
      
      <guid>https://blog.pydaxing.com/post/20231018_codellama/</guid>
      <description>1、简介 2023年，Meta AI发布了一系列的Code Llama，这些Code Llama都是基于Llama 2的大型语言模型，专门用于生成代码。该项目提供了多个版本的模型，包括基础模型（Code Llama）、针对Python的专业化模型（Code Llama - Python）以及可以根据指令进行操作的模型（Code Llama - Instruct）。这些模型在开放模型领域提供了最先进的性能、填充能力，能够支持大规模的输入上下文，并具备零样本学习（zero-shot learning）编程任务指令跟随的能力。
2、背景 使用大规模的领域内数据集可以显著提高模型在自然语言和领域特定语言、专业术语等方面的理解能力，进而在程序合成、代码补全、调试和生成文档等方向的应用中取得更好的性能。目前已有的代码大模型方案存在一些问题，而Codellama提供了相应的解决方案。
代码能力训练： 基础模型如AlphaCode、InCoder和StarCoder等模型只在代码数据上进行预训练，而Codex是基于通用语言模型微调而来的。相比于只在代码数据上进行预训练的模型，Codellama基于Llama2在通用数据和代码数据上进行了预训练，实验证明其性能更好。
代码填充/补全： LLMs的自回归训练和微调适用于提示完成，但在考虑完整的上下文情况下填补缺失的文本部分方面存在一定的限制。为了解决这个问题，Codellama采用了多任务训练损失，包括自回归和因果填充预测，使得Codellama可以在源代码编辑器中实现实时补全和文档字符串生成等应用。
长上下文： llama2支持的4096个token输入序列长度在代码领域可能不够。为了解决这个问题，Codellama提出了一个额外的微调阶段，通过修改llama2使用的RoPE位置编码，将序列长度从4096扩展到100,000。
指令微调： 指令微调有助于避免生成不安全、有毒或带有偏见的代码。Codellama的-instruct变种进一步在混合专有指令数据上进行微调，以提高安全性和实用性；通过采用self-instruct的方式生成一个新的数据集，其中llama2用于生成问题，而Code Llama用于生成相应的单元测试和解决方案。
3、Code LlaMA方案 3.1、Code LlaMA系列 Code Llama提供了在所有开源模型中的最先进性能、填充能力、支持大输入上下文以及zero-shot编程任务指令跟随能力。它提供了多个版本，以满足不同应用的需求：
基础模型（Code Llama）： 参数为7B，用于通用代码生成任务。 用于代码生成的基础模型，有三种参数大小：7B、13B和34B。 7B和13B模型使用填充目标进行训练，适用于在集成开发环境（IDE）中补全代码等中间任务。 34B模型没有使用填充目标训练。 所有模型都是在Llama2基础上初始化，并使用了500B的训练数据。 经过长文本微调的训练。 Python专业化版本（Code Llama - Python）： 参数为13B，针对Python编程的任务进行了优化。 专门用于Python代码生成，有三种大小的模型参数：7B、13B和34B。 旨在研究针对单一编程语言定制的模型与通用代码生成模型在性能上的差异。 所有模型都是从Llama2基础上初始化，并首先进行500B数据的预训练，然后使用100B的Python数据进行专门强化。 所有Code Llama-Python模型都没有使用填充填充目标训。 经过长文本微调的训练。 指令模型（Code Llama - Instruct）： 参数为34B，支持根据指令进行代码生成。 基于Code Llama初始化，经过额外的5B token训练数据微调，以更好地遵循人类指令。 每个版本都在16k token序列上进行训练，并对最多包含100k token的输入进行了长序列改进。7B和13B的Code Llama以及Code Llama - Instruct等变种支持填充补全能力。Code Llama在多个代码基准测试中展现了开放模型中的最先进性能，特别是在HumanEval和MBPP测试中，分数分别达到53%和55%（优于Llama2 70B模型）。而在多语言MultiPL-E测试中，Code Llama的精度也超过了所有开源模型。
最重要的是Code Llama遵循与Llama2相同的开源协议，并允许进行研究和商业用途。</description>
    </item>
    
    <item>
      <title>一文从入门到精通Prompt工程</title>
      <link>https://blog.pydaxing.com/post/prompt-%E5%B7%A5%E7%A8%8B%E8%B0%83%E7%A0%94/</link>
      <pubDate>Sat, 15 Apr 2023 18:12:17 +0000</pubDate>
      
      <guid>https://blog.pydaxing.com/post/prompt-%E5%B7%A5%E7%A8%8B%E8%B0%83%E7%A0%94/</guid>
      <description>最近在了解大语言模型相关一些技术，Prompt工程成了一个比较热的话题，甚至有公司还有prompt工程师，所以在这里对prompt做一个系统性的调研和总结，一起学习下。
1、Prompt工程及必要性 1.1、什么是 prompt NLP之前的发展可以概括为特征工程、结构工程和目标工程。
特征工程：监督学习依赖特征的好坏，深度学习之前需要相关研究人员或者专业人士利用自己扎实的领域知识从原始数据中定义并提取有用的特征供 模型学习。
结构工程：深度学习出现之后特征能从数据中直接学习到，所以聚焦于如何设计一个合理的网络结果去学习有用的特征，从而减少对人工构建特征的依赖。
目标工程：在预训练+微调出现之后，由于预训练不需要专家知识，可直接在大规模文本上训练。因此又聚焦目标的设计，合理设计预训练跟微调阶段的目标函数，对最终的效果影响深远。
预训练任务是在大规模语言文本数据上进行的，不需要专业领域知识，训练方式是让模型补全上下文（可以理解为完形填空），因此模型的目标就是槽填充（Slot Filting）。在目标工程的过程中，发现下游任务目标与预训练目标对齐是最友好的。因此在下游任务中通过引入“文本提示符（textual prompt）”，把下游任务的目标重构成跟预训练一致的上下文补全是最好的选择。
比如对于一个输入 “I missed the bus today.” ，分别在分类任务和翻译任务下向预训练任务对齐，可以重构如下
情感预测任务
原任务：I missed the bus today. -&amp;gt; positive / negative
原目标：分类预测
重构任务：I missed the bus today. It is a ___ day.
重构目标：槽填充（Slot Filting）
提示词（prompt）：It is a day
语言翻译任务
原任务：I missed the bus today -&amp;gt; J&amp;rsquo;ai raté le bus aujourd&amp;rsquo;hui
原目标：回归预测
重构任务：English : I missed the bus today. French: ___.
重构目标：槽填充（Slot Filting）</description>
    </item>
    
  </channel>
</rss>
