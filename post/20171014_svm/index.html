<!doctype html>
































<html
  class="not-ready lg:text-base"
  style="--bg: #fbfbfb"
  lang="zh-cn"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>支持向量机 - 冷眸</title>

  
  <meta name="theme-color" />

  
  
  
  <meta name="description" content="支持向量机（SVM）是一个非常好的监督学习算法，由于这个算法涉及到的数学知识比较多，而且不容易弄懂，因此我额外补了一些拉格朗日对偶性的一些知识，还初步了解了以下凸优化的相关知识，建议如过想学SVM的先去学一些拉格朗日对偶问题以及一些凸优化的相关东西。终于能大致弄懂SVM的整个过程中所涉及到的一些东西，但是仍有一些地方还是不太明白，还有待继续学习。这里先对所掌握的过程进行回顾总结。
Support Vector Machine (SVM) 支持向量机（SVM）是一个监督学习算法，它实质上是一种特征空间上的间隔最大的线性分类器，这其中涉及到核技巧将线性不可分问题映射成特征空间中的线性可分问题，因此支持向量机可以说是一个非线性分类器，它的学习策略就是间隔最大化，最后的问题可以转化为一个凸优化问题，使用拉格朗日对偶性来求解对偶问题，从而间接得到原问题的解，一般使用SMO算法来求对偶问题的解。
间隔最大化 支持向量机的思想就是间隔最大化，根据间隔最大化来构造原始问题。这里涉及到的间隔包括函数间隔和几何间隔，其中函数间隔为$\hat \gamma_i$ $$ \hat \gamma_i = y_i(\omega\cdot x_i &#43; b) \tag{1} $$ 然后得到所有样本的函数间隔中的最小的一个$$\hat\gamma = \min_{i = 1,\cdots,N}(\hat\gamma_i)$$
几何间隔由空间中两点的实际距离定义记作$\gamma_i$ $$ \gamma_i = \dfrac{\omega}{||\omega||}\cdot x_i &#43; \dfrac{b}{||\omega||}\tag{2} $$ 同理求出所有几何间隔最小的$$\gamma = \min_{i =1\cdots,N}(\gamma_i)$$
可以看出函数间隔是不唯一的，因为满足分离超平面$\omega^\ast\cdot x &#43; b^\ast= 0$的参数$(\omega,b)$并不是唯一的，例如将$\omega$和$b$发你别扩大两倍后$2\omega^\ast\cdot x &#43; 2b^\ast = 0$仍然成立，而函数间隔$\hat\gamma$会随着$(\omega , b)$变化。而几何距离却不会发生变化。
同时还可以看出函数间隔和几何间隔之间满足关系(3) $$ \gamma = \dfrac{\hat\gamma}{||\omega||}\tag{3} $$ 因此这里考虑求解几何间隔最大的分离超平面，该问题则表示为下面的约束最优化问题 $$ \max_{\omega,b} \quad \gamma\tag{4}\\ s.t.\quad y_i(\dfrac{\omega}{||\omega||}\cdot x_i &#43; \dfrac{b}{||\omega||}) \ge \gamma,\qquad i = 1,2,\cdots,N $$ 根据上述关系(3)将问题转换为 $$ \max_{\omega,b} \quad \dfrac{\hat\gamma}{||\omega||}\tag{5}\\ s." />
  <meta name="author" content="冷眸" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://blog.pydaxing.com/main.min.css" />

  
  <script
    defer
    src="https://blog.pydaxing.com/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
     
  <link rel="preload" as="image" href="https://blog.pydaxing.com/theme.svg" />

  
  
  
  <link rel="preload" as="image" href="https://blog.pydaxing.com/avator.png" />
  
  

  
  

  
  
  <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>

<script>
    document.addEventListener("DOMContentLoaded", () =>
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
          ],
          
          throwOnError : false
        })
    );
</script>

  
  
  

  
  <link rel="icon" href="https://blog.pydaxing.com/avator.png" />
  <link rel="apple-touch-icon" href="https://blog.pydaxing.com/apple-touch-icon.png" />


  
  <meta name="generator" content="Hugo 0.118.2">

  
  

  
  
  
  
  
  <meta itemprop="name" content="支持向量机">
<meta itemprop="description" content="支持向量机（SVM）是一个非常好的监督学习算法，由于这个算法涉及到的数学知识比较多，而且不容易弄懂，因此我额外补了一些拉格朗日对偶性的一些知识，还初步了解了以下凸优化的相关知识，建议如过想学SVM的先去学一些拉格朗日对偶问题以及一些凸优化的相关东西。终于能大致弄懂SVM的整个过程中所涉及到的一些东西，但是仍有一些地方还是不太明白，还有待继续学习。这里先对所掌握的过程进行回顾总结。
Support Vector Machine (SVM) 支持向量机（SVM）是一个监督学习算法，它实质上是一种特征空间上的间隔最大的线性分类器，这其中涉及到核技巧将线性不可分问题映射成特征空间中的线性可分问题，因此支持向量机可以说是一个非线性分类器，它的学习策略就是间隔最大化，最后的问题可以转化为一个凸优化问题，使用拉格朗日对偶性来求解对偶问题，从而间接得到原问题的解，一般使用SMO算法来求对偶问题的解。
间隔最大化 支持向量机的思想就是间隔最大化，根据间隔最大化来构造原始问题。这里涉及到的间隔包括函数间隔和几何间隔，其中函数间隔为$\hat \gamma_i$ $$ \hat \gamma_i = y_i(\omega\cdot x_i &#43; b) \tag{1} $$ 然后得到所有样本的函数间隔中的最小的一个$$\hat\gamma = \min_{i = 1,\cdots,N}(\hat\gamma_i)$$
几何间隔由空间中两点的实际距离定义记作$\gamma_i$ $$ \gamma_i = \dfrac{\omega}{||\omega||}\cdot x_i &#43; \dfrac{b}{||\omega||}\tag{2} $$ 同理求出所有几何间隔最小的$$\gamma = \min_{i =1\cdots,N}(\gamma_i)$$
可以看出函数间隔是不唯一的，因为满足分离超平面$\omega^\ast\cdot x &#43; b^\ast= 0$的参数$(\omega,b)$并不是唯一的，例如将$\omega$和$b$发你别扩大两倍后$2\omega^\ast\cdot x &#43; 2b^\ast = 0$仍然成立，而函数间隔$\hat\gamma$会随着$(\omega , b)$变化。而几何距离却不会发生变化。
同时还可以看出函数间隔和几何间隔之间满足关系(3) $$ \gamma = \dfrac{\hat\gamma}{||\omega||}\tag{3} $$ 因此这里考虑求解几何间隔最大的分离超平面，该问题则表示为下面的约束最优化问题 $$ \max_{\omega,b} \quad \gamma\tag{4}\\ s.t.\quad y_i(\dfrac{\omega}{||\omega||}\cdot x_i &#43; \dfrac{b}{||\omega||}) \ge \gamma,\qquad i = 1,2,\cdots,N $$ 根据上述关系(3)将问题转换为 $$ \max_{\omega,b} \quad \dfrac{\hat\gamma}{||\omega||}\tag{5}\\ s."><meta itemprop="datePublished" content="2017-10-14T12:18:00+00:00" />
<meta itemprop="dateModified" content="2017-10-14T12:18:00+00:00" />
<meta itemprop="wordCount" content="355">
<meta itemprop="keywords" content="机器学习," />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="支持向量机"/>
<meta name="twitter:description" content="支持向量机（SVM）是一个非常好的监督学习算法，由于这个算法涉及到的数学知识比较多，而且不容易弄懂，因此我额外补了一些拉格朗日对偶性的一些知识，还初步了解了以下凸优化的相关知识，建议如过想学SVM的先去学一些拉格朗日对偶问题以及一些凸优化的相关东西。终于能大致弄懂SVM的整个过程中所涉及到的一些东西，但是仍有一些地方还是不太明白，还有待继续学习。这里先对所掌握的过程进行回顾总结。
Support Vector Machine (SVM) 支持向量机（SVM）是一个监督学习算法，它实质上是一种特征空间上的间隔最大的线性分类器，这其中涉及到核技巧将线性不可分问题映射成特征空间中的线性可分问题，因此支持向量机可以说是一个非线性分类器，它的学习策略就是间隔最大化，最后的问题可以转化为一个凸优化问题，使用拉格朗日对偶性来求解对偶问题，从而间接得到原问题的解，一般使用SMO算法来求对偶问题的解。
间隔最大化 支持向量机的思想就是间隔最大化，根据间隔最大化来构造原始问题。这里涉及到的间隔包括函数间隔和几何间隔，其中函数间隔为$\hat \gamma_i$ $$ \hat \gamma_i = y_i(\omega\cdot x_i &#43; b) \tag{1} $$ 然后得到所有样本的函数间隔中的最小的一个$$\hat\gamma = \min_{i = 1,\cdots,N}(\hat\gamma_i)$$
几何间隔由空间中两点的实际距离定义记作$\gamma_i$ $$ \gamma_i = \dfrac{\omega}{||\omega||}\cdot x_i &#43; \dfrac{b}{||\omega||}\tag{2} $$ 同理求出所有几何间隔最小的$$\gamma = \min_{i =1\cdots,N}(\gamma_i)$$
可以看出函数间隔是不唯一的，因为满足分离超平面$\omega^\ast\cdot x &#43; b^\ast= 0$的参数$(\omega,b)$并不是唯一的，例如将$\omega$和$b$发你别扩大两倍后$2\omega^\ast\cdot x &#43; 2b^\ast = 0$仍然成立，而函数间隔$\hat\gamma$会随着$(\omega , b)$变化。而几何距离却不会发生变化。
同时还可以看出函数间隔和几何间隔之间满足关系(3) $$ \gamma = \dfrac{\hat\gamma}{||\omega||}\tag{3} $$ 因此这里考虑求解几何间隔最大的分离超平面，该问题则表示为下面的约束最优化问题 $$ \max_{\omega,b} \quad \gamma\tag{4}\\ s.t.\quad y_i(\dfrac{\omega}{||\omega||}\cdot x_i &#43; \dfrac{b}{||\omega||}) \ge \gamma,\qquad i = 1,2,\cdots,N $$ 根据上述关系(3)将问题转换为 $$ \max_{\omega,b} \quad \dfrac{\hat\gamma}{||\omega||}\tag{5}\\ s."/>

  
  


  <link
          rel="stylesheet"
          href="https://unpkg.com/@waline/client@v2/dist/waline.css"
  />
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold"
      href="https://blog.pydaxing.com"
      >冷眸</a
    >
    <div
      class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  
  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#fbfbfb'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/"
        >文章</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/tags/"
        >标签</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/archive/"
        >归档</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >关于</a
      >
      
    </nav>
    

    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"
    >
      

<article>
  <header class="mb-16">
    <h1 class="!my-0 pb-2.5">支持向量机</h1>

    
    <div class="text-sm antialiased opacity-60">
      
      <time>Oct 14, 2017</time>
      
      
      
      
      <span class="mx-1">&middot;</span>
      <span>冷眸</span>
      
    </div>
    
  </header>

  <div class="toc mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]">
  <details >
  <summary accesskey="c" title="(Alt + C)">
    <span class="details" style="font-weight: bold; font-size: 18px">目 录</span>
  </summary>

  <div class="inner"><ul>
      <li>
        <a href="#support-vector-machine-svm" aria-label="Support Vector Machine (SVM)">Support Vector Machine (SVM)</a><ul>
          
      <li>
        <a href="#%e9%97%b4%e9%9a%94%e6%9c%80%e5%a4%a7%e5%8c%96" aria-label="间隔最大化">间隔最大化</a></li>
      <li>
        <a href="#%e6%8b%89%e6%a0%bc%e6%9c%97%e6%97%a5%e5%af%b9%e5%81%b6%e6%80%a7" aria-label="拉格朗日对偶性">拉格朗日对偶性</a></li>
      <li>
        <a href="#%e9%9d%9e%e7%ba%bf%e6%80%a7%e6%94%af%e6%8c%81%e5%90%91%e9%87%8f%e6%9c%ba%e4%b8%8e%e6%a0%b8%e6%8a%80%e5%b7%a7" aria-label="非线性支持向量机与核技巧">非线性支持向量机与核技巧</a></li>
      <li>
        <a href="#%e5%ba%8f%e5%88%97%e6%9c%80%e5%b0%8f%e6%9c%80%e4%bc%98%e5%8c%96%e7%ae%97%e6%b3%95smo" aria-label="序列最小最优化算法（SMO）">序列最小最优化算法（SMO）</a><ul>
          
      <li>
        <a href="#%e4%b8%a4%e4%b8%aa%e5%8f%98%e9%87%8f%e7%9a%84%e4%ba%8c%e6%ac%a1%e8%a7%84%e5%88%92%e8%a7%a3%e6%9e%90%e6%96%b9%e6%b3%95" aria-label="两个变量的二次规划解析方法">两个变量的二次规划解析方法</a></li>
      <li>
        <a href="#%e5%90%af%e5%8f%91%e5%bc%8f%e5%8f%98%e9%87%8f%e9%80%89%e6%8b%a9%e6%96%b9%e6%b3%95" aria-label="启发式变量选择方法">启发式变量选择方法</a></li></ul>
      </li>
      <li>
        <a href="#%e6%80%bb%e7%bb%93" aria-label="总结">总结</a>
      </li>
    </ul>
    </li>
    </ul>
  </div>
  </details>
</div>
  

  <section><p>支持向量机（SVM）是一个非常好的监督学习算法，由于这个算法涉及到的数学知识比较多，而且不容易弄懂，因此我额外补了一些拉格朗日对偶性的一些知识，还初步了解了以下凸优化的相关知识，建议如过想学SVM的先去学一些拉格朗日对偶问题以及一些凸优化的相关东西。终于能大致弄懂SVM的整个过程中所涉及到的一些东西，但是仍有一些地方还是不太明白，还有待继续学习。这里先对所掌握的过程进行回顾总结。</p>
<!-- more -->
<h2 id="support-vector-machine-svm">Support Vector Machine (SVM)</h2>
<p>支持向量机（SVM）是一个监督学习算法，它实质上是一种特征空间上的间隔最大的线性分类器，这其中涉及到核技巧将线性不可分问题映射成特征空间中的线性可分问题，因此支持向量机可以说是一个非线性分类器，它的学习策略就是间隔最大化，最后的问题可以转化为一个凸优化问题，使用拉格朗日对偶性来求解对偶问题，从而间接得到原问题的解，一般使用SMO算法来求对偶问题的解。</p>
<h3 id="间隔最大化">间隔最大化</h3>
<p>支持向量机的思想就是间隔最大化，根据间隔最大化来构造原始问题。这里涉及到的间隔包括函数间隔和几何间隔，其中函数间隔为$\hat \gamma_i$
$$
\hat \gamma_i = y_i(\omega\cdot x_i + b) \tag{1}
$$
然后得到所有样本的函数间隔中的最小的一个$$\hat\gamma = \min_{i = 1,\cdots,N}(\hat\gamma_i)$$</p>
<p>几何间隔由空间中两点的实际距离定义记作$\gamma_i$
$$
\gamma_i = \dfrac{\omega}{||\omega||}\cdot x_i + \dfrac{b}{||\omega||}\tag{2}
$$
同理求出所有几何间隔最小的$$\gamma = \min_{i =1\cdots,N}(\gamma_i)$$</p>
<p>可以看出函数间隔是不唯一的，因为满足分离超平面$\omega^\ast\cdot x + b^\ast= 0$的参数$(\omega,b)$并不是唯一的，例如将$\omega$和$b$发你别扩大两倍后$2\omega^\ast\cdot x + 2b^\ast = 0$仍然成立，而函数间隔$\hat\gamma$会随着$(\omega , b)$变化。而几何距离却不会发生变化。</p>
<p>同时还可以看出函数间隔和几何间隔之间满足关系(3)
$$
\gamma = \dfrac{\hat\gamma}{||\omega||}\tag{3}
$$
因此这里考虑求解几何间隔最大的分离超平面，该问题则表示为下面的约束最优化问题
$$
\max_{\omega,b} \quad \gamma\tag{4}\\
s.t.\quad y_i(\dfrac{\omega}{||\omega||}\cdot x_i + \dfrac{b}{||\omega||}) \ge \gamma,\qquad i = 1,2,\cdots,N
$$
根据上述关系(3)将问题转换为
$$
\max_{\omega,b} \quad \dfrac{\hat\gamma}{||\omega||}\tag{5}\\
s.t.\quad y_i(\omega\cdot x_i + b) \ge \hat\gamma,\qquad i = 1,2,\cdots,N
$$
因为函数距离跟随$\omega$和$b$而变化，这里做出限制$\hat\gamma = 1$。而最大化$\dfrac{1}{||\omega||}$和最小化$\dfrac{1}{2}||\omega||^2$是等价的，因此，问题进一步转化为
$$
\min_{\omega, b}\quad\dfrac{1}{2}||\omega||^2\tag{6}\\
s.t.\quad y_i(\omega\cdot x_i + b) \ge 1,\qquad i = 1,2,\cdots,N
$$
问题(6)就是支持向量机需要解决的问题，即原始最优化问题，最终我们求得的分类决策函数是
$$
f(x) = \text{sign}(\omega^\ast\cdot x + b^\ast)\tag{7}
$$</p>
<h3 id="拉格朗日对偶性">拉格朗日对偶性</h3>
<p>对上述问题(6)建立拉格朗日函数，引入拉格朗日乘子$\alpha_i \ge 0, \quad i = 1,2,\cdots,N$，定义拉格朗日函数为
$$
L(\omega,b,\alpha) = \dfrac{1}{2}||\omega||^2 - \sum_{i=1}^N\alpha_iy_i(\omega\cdot x_i + b) + \sum_{i = 1}^N\alpha_i\tag{8}
$$
分别对$\omega$和$b$求偏导令其为零然后带入到拉格朗日函数(8)中去，即得到其对偶问题(关于对偶的相关知识不在此陈述)
$$
\min_{\alpha}\quad\dfrac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i = 1}^N\alpha_i\tag{9}\\
s.t.\quad \sum_{i = 1}^N\alpha_iy_i = 0\\
\alpha_i \ge 0, \quad i = 1,2,\cdots, N
$$
然而实际情况中，线性可分的情况是十分少的，一般都是近似线性可分的的，这里涉及到的是软间隔支持向量机，也称为线性支持向量机，是最基本的支持向量机。</p>
<p>对与近似线性可分的样本中的噪声样本通过引入一个松弛变量$\xi_i$，从而使其在一定的可接受的误差范围内可分，因此得到线性支持向量机的原始最优化问题
$$
\min_{\omega, b}\quad\dfrac{1}{2}||\omega||^2+C\sum_{i = 1}^N\xi_i\tag{10}\\
s.t.\quad y_i(\omega\cdot x_i + b) \ge 1 - \xi_i,\qquad i = 1,2,\cdots,N\\
\xi_i \ge 0, \quad i = 1,2,\cdots,N
$$
其对偶问题为
$$
\min_{\alpha}\quad\dfrac{1}{2}\sum_{i = 1}^{N}\sum_{j = 1}^N\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i = 1}^N\alpha_i\tag{11}\\
s.t.\quad \sum_{i = 1}^N\alpha_iy_i = 0\\
0 \le\alpha_i \le C, \quad i = 1,2,\cdots, N
$$
可一看到，他和线性可分支持向量机的唯一区别是$\alpha_i$的约束多了一个上限。</p>
<h3 id="非线性支持向量机与核技巧">非线性支持向量机与核技巧</h3>
<p>对于输入空间中的分线性分类问题，可以通过非线性变换将它转换为某个高维特征空间中的线性分类问题，在高维空间中学习线性支持向量机。</p>
<p>输入空间到某个高维空间的转换是通过映射$\phi(x):\chi \to H$来完成的，对于$\forall x,z \in \chi$，函数$K(x,z)$满足条件$K(x,z) = \phi(x)\cdot\phi(z)$，则称函数$K(x,z)$为核函数。</p>
<p>目标函数与分类决策函数中都只涉及实例与实例之间的内积，因此并不需要显示的指定非线性变换，而是直接用核函数来代替内积，对偶问题则变为如下形式
$$
W(\alpha) = \dfrac{1}{2}\sum_{i = 1}^N\sum_{j = 1}^N\alpha_i\alpha_jy_iy_jK(x_i,x_j) - \sum_{i = 1}^N\alpha_i\tag{12}
$$
分类决策函数中的内积也用核函数来表示，则
$$
f(x) = \text{sign}(\sum_{i = 1}^{N_s}\alpha_i^{\ast}y_i\phi(x_i)\cdot\phi(x)+b^\ast) = \text{sign}(\sum_{i = 1}^{N_s}\alpha_i^\ast y_iK(x_i,x)+b^\ast)\tag{13}
$$
因此在线性支持向量机学习的对偶问题中使用核函数来代替内积即可解得非线性支持向量机。</p>
<h3 id="序列最小最优化算法smo">序列最小最优化算法（SMO）</h3>
<p>该算法分为两个部分分别是</p>
<blockquote>
<p>求解两个变量二次规划的解析方法</p>
<p>选择变量的启发式方法</p>
</blockquote>
<h4 id="两个变量的二次规划解析方法">两个变量的二次规划解析方法</h4>
<p>假定选择两个变量$\alpha_1$和$\alpha_2$，而其它的变量$\alpha_i(i = 3,4,\cdots,N)$保持不变，则(12)式的子问题可以表示为
$$
\min_{\alpha_1,\alpha_2}\quad W(\alpha_1,\alpha_2) = \dfrac{1}{2}K_{11}\alpha_1^2+\dfrac{1}{2}K_{22}\alpha_2^2 + y_1y_2K_{12}\alpha_1\alpha_2 \\ \quad\quad\quad- (\alpha_1+\alpha_2) + y_1\alpha_1\sum_{i = 3}^{N}y_i\alpha_iK_{i1} + y_2\alpha_2\sum_{i = 3}^Ny_i\alpha_iK_{i2}\tag{14}\\s.t.\quad \alpha_1y_1 + \alpha_2y_2 = -\sum_{i = 3}^Ny_i\alpha_i = \zeta\\0 \le \alpha\le C,\quad i = 1,2
$$
将问题(14)中的等式约束$\alpha_1y_1 + \alpha_2y_2 = -\sum_{i = 3}^Ny_i\alpha_i = \zeta$进行变换，用$\alpha_2$来表示$\alpha_1$，将其代入到(14)式中，那么问题就变为关于变量$\alpha_2$的二次函数求极值的问题，求得$\alpha_2$之后通过等式$\alpha_1y_1 + \alpha_2y_2 = -\sum_{i = 3}^Ny_i\alpha_i = \zeta$即可得到$\alpha_1$的值。</p>
<h4 id="启发式变量选择方法">启发式变量选择方法</h4>
<p>SMO称第一个变量的选择为外层循环，外层循环在训练样本中选取<code>违反KKT条件最严重</code>的样本点，并将其对应的变量作为第一个变量，具体的检验训练样本$(x_i, y_i)$是否满足KKT条件，即
$$
\alpha_i = 0 \Leftrightarrow y_i g(x_i) \ge 1\\ 0 \lt \alpha_i \lt C \Leftrightarrow y_ig(x_i) = 1\\ \alpha_i = C \Leftrightarrow y_ig(x_i) \le 1
$$
其中$g(x_i) = \sum_{j = 1}^N\alpha_jy_jK(x_i,x_j) + b$，改检验是在$\varepsilon$范围内进行的。</p>
<p>第二个变量的选择称为内层循环，在找到第一个变量$\alpha_1$的条件下，在内层循环中寻找第二个变量$\alpha_2$，第二个变量的选择标准是希望能够使$\alpha_2$尽量有足够大的变化。</p>
<p>关于SMO算法中的变量选择方法我目前并不是完全懂，先写到这里为止。</p>
<h3 id="总结">总结</h3>
<p>此次总结对之前学的支持向量机的相关知识进行了一次回顾，支持向量机结合核技巧是一个非常好的学习算法，它利用了拉格朗日问题的对偶性，将原问题转换为对偶问题，因为可能存在一种情况是每个样本的属性非常的多，也就是输入空间的维数非常的大，而转换为对偶问题之后，参数是和样本一一对应的，因此参数数目就是样本的数目，这能够大大减小计算的复杂度，最后对于求解凸二次规划问题有十分有效的SMO算法，因为子问题有解析解，因此每次计算子问题都十分的快，虽然子问题数目多，但是总体上看来还是很快的。</p>
</section>

  
  
  <footer class="mt-12 flex flex-wrap">
     
    <a
      class="mb-1.5 mr-1.5 rounded-lg bg-black/[3%] px-5 py-1.5 no-underline dark:bg-white/[8%]"
      href="https://blog.pydaxing.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"
      >机器学习</a
    >
    
  </footer>
  

  
  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]"
      href="https://blog.pydaxing.com/post/20171226_nwpureview/"
      ><span class="mr-1.5">←</span><span>大学四年总结</span></a
    >
    
    
  </nav>
  
  
  
  <div style="margin-bottom: 5em"></div>
  <div id="waline"></div>
  <script type="module">
    import { init } from 'https://unpkg.com/@waline/client@v2/dist/waline.mjs';

    init({
      el: '#waline',
      lang: 'zh-CN',
      dark: 'auto',
      meta: ['nick', 'mail'],
      requiredMeta: ['nick', 'mail'],
      login: 'force',
      wordLimit: 100,
      pageSize: 10,
      avatar: 'wavatar',
      serverURL: 'https://pydaxing.netlify.app/.netlify/functions/comment/',
    });
  </script>
  
  
  

  
  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2023
    <a class="link" href="https://blog.pydaxing.com">冷眸</a>
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >Powered by Hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >✎ Paper</a
  >
</footer>

  </body>
</html>
